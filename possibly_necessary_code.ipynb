{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "possibly_necessary_code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMKRUqIC3vO6p6HMrk7uSyl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/El-Nico/tensorflow-notes/blob/main/possibly_necessary_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###data preprocessing"
      ],
      "metadata": {
        "id": "MkYEgS0DYTGS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####dealing with zip files"
      ],
      "metadata": {
        "id": "89nCWguyY8uZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Download zip file of pizza_steak images\n",
        "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/pizza_steak.zip \n",
        "\n",
        "# Unzip the downloaded file\n",
        "zip_ref = zipfile.ZipFile(\"pizza_steak.zip\", \"r\")\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()\n",
        "\n",
        "!ls pizza_steak\n",
        "\n",
        "!ls pizza_steak/train/\n",
        "\n",
        "!ls pizza_steak/train/steak/"
      ],
      "metadata": {
        "id": "ZICxTBx6ZKJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example of file structure\n",
        "\n",
        "pizza_steak <- top level folder\n",
        "└───train <- training images\n",
        "│   └───pizza\n",
        "│   │   │   1008104.jpg\n",
        "│   │   │   1638227.jpg\n",
        "│   │   │   ...      \n",
        "│   └───steak\n",
        "│       │   1000205.jpg\n",
        "│       │   1647351.jpg\n",
        "│       │   ...\n",
        "│   \n",
        "└───test <- testing images\n",
        "│   └───pizza\n",
        "│   │   │   1001116.jpg\n",
        "│   │   │   1507019.jpg\n",
        "│   │   │   ...      \n",
        "│   └───steak\n",
        "│       │   100274.jpg\n",
        "│       │   1653815.jpg\n",
        "│       │   ..."
      ],
      "metadata": {
        "id": "INLsuypzZbIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Walk through 10_food_classes directory and list number of files\n",
        "for dirpath, dirnames, filenames in os.walk(\"10_food_classes_all_data\"):\n",
        "  print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")\n",
        "\n",
        "  ############################################################################\n",
        "  # Get the class names for our multi-class dataset\n",
        "import pathlib\n",
        "import numpy as np\n",
        "data_dir = pathlib.Path(train_dir)\n",
        "class_names = np.array(sorted([item.name for item in data_dir.glob('*')]))\n",
        "print(class_names)\n",
        "\n",
        "#######################################################\n",
        "# View a random image from the training dataset\n",
        "import random\n",
        "img = view_random_image(target_dir=train_dir,\n",
        "                        target_class=random.choice(class_names)) # get a random class name"
      ],
      "metadata": {
        "id": "JjvWwB-nfXhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###learning rate"
      ],
      "metadata": {
        "id": "jVVgVNKTpNbx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFt0CujIpHmd"
      },
      "outputs": [],
      "source": [
        "#creating a learning rate scheduler callback\n",
        "#lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 * 10**(epoch/20))# traverse a set of learning rates strating from 1e-4, increasing by 10**(epoch/20) every epoch\n",
        "\n",
        "#fit the model for 100 epochs\n",
        "# history=fashion.fit(train_images,\n",
        "#                     train_labels, epochs=50, \n",
        "#                     validation_data=(test_images, test_labels),\n",
        "#                     callbacks=[lr_scheduler])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checkout the history\n",
        "#pd.DataFrame(history.history).plot(figsize=(10,7), xlabel=\"epochs\");"
      ],
      "metadata": {
        "id": "AqjR61e9piO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the learning rate versus the loss\n",
        "# lrs = 1e-4 * (10 ** (np.arange(100)/20))\n",
        "# plt.figure(figsize=(10, 7))\n",
        "# plt.semilogx(lrs, history.history[\"loss\"]) # we want the x-axis (learning rate) to be log scale\n",
        "# plt.xlabel(\"Learning Rate\")\n",
        "# plt.ylabel(\"Loss\")\n",
        "# plt.title(\"Learning rate vs. loss\");"
      ],
      "metadata": {
        "id": "bwu4a623pyfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![how to select the best learning rate](https://raw.githubusercontent.com/El-Nico/tensorflow-notes/main/tensorflow_notes_images/learning_rate.png)"
      ],
      "metadata": {
        "id": "31ctrT8Gw1Q8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###creating a confusion matrix"
      ],
      "metadata": {
        "id": "i8yD7H-Sx5PO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# y_pred=fashion_non_normal.predict(test_images)\n",
        "\n",
        "# y_pred.shape\n"
      ],
      "metadata": {
        "id": "MoDJ_4l2IW25",
        "outputId": "354405e3-6d3f-4815-a332-2587d5fdc739",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#convert the prediction from probs to labels\n",
        "# y_pred=y_pred.argmax(axis=-1)\n",
        "# y_pred.shape"
      ],
      "metadata": {
        "id": "GudYjyf3KB_t",
        "outputId": "512a5131-f09b-4c75-b33e-5d9184bf52f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000,)"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def make_confusion_matrix(y_true, y_pred, classes=None, figsize=(10, 10), text_size=15): \n",
        "  \"\"\"Makes a labelled confusion matrix comparing predictions and ground truth labels.\n",
        "\n",
        "  If classes is passed, confusion matrix will be labelled, if not, integer class values\n",
        "  will be used.\n",
        "\n",
        "  Args:\n",
        "    y_true: Array of truth labels (must be same shape as y_pred).\n",
        "    y_pred: Array of predicted labels (must be same shape as y_true).\n",
        "    classes: Array of class labels (e.g. string form). If `None`, integer labels are used.\n",
        "    figsize: Size of output figure (default=(10, 10)).\n",
        "    text_size: Size of output figure text (default=15).\n",
        "  \n",
        "  Returns:\n",
        "    A labelled confusion matrix plot comparing y_true and y_pred.\n",
        "\n",
        "  Example usage:\n",
        "    make_confusion_matrix(y_true=test_labels, # ground truth test labels\n",
        "                          y_pred=y_preds, # predicted labels\n",
        "                          classes=class_names, # array of class label names\n",
        "                          figsize=(15, 15),\n",
        "                          text_size=10)\n",
        "  \"\"\"  \n",
        "  # Create the confustion matrix\n",
        "  cm = confusion_matrix(y_true, y_pred)\n",
        "  cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis] # normalize it\n",
        "  n_classes = cm.shape[0] # find the number of classes we're dealing with\n",
        "\n",
        "  # Plot the figure and make it pretty\n",
        "  fig, ax = plt.subplots(figsize=figsize)\n",
        "  cax = ax.matshow(cm, cmap=plt.cm.Blues) # colors will represent how 'correct' a class is, darker == better\n",
        "  fig.colorbar(cax)\n",
        "\n",
        "  # Are there a list of classes?\n",
        "  if classes:\n",
        "    labels = classes\n",
        "  else:\n",
        "    labels = np.arange(cm.shape[0])\n",
        "  \n",
        "  # Label the axes\n",
        "  ax.set(title=\"Confusion Matrix\",\n",
        "         xlabel=\"Predicted label\",\n",
        "         ylabel=\"True label\",\n",
        "         xticks=np.arange(n_classes), # create enough axis slots for each class\n",
        "         yticks=np.arange(n_classes), \n",
        "         xticklabels=labels, # axes will labeled with class names (if they exist) or ints\n",
        "         yticklabels=labels)\n",
        "  \n",
        "  # Make x-axis labels appear on bottom\n",
        "  ax.xaxis.set_label_position(\"bottom\")\n",
        "  ax.xaxis.tick_bottom()\n",
        "\n",
        "  # Set the threshold for different colors\n",
        "  threshold = (cm.max() + cm.min()) / 2.\n",
        "\n",
        "  # Plot the text on each cell\n",
        "  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "    plt.text(j, i, f\"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)\",\n",
        "             horizontalalignment=\"center\",\n",
        "             color=\"white\" if cm[i, j] > threshold else \"black\",\n",
        "             size=text_size)"
      ],
      "metadata": {
        "id": "Q9SzFA4tHBuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###viewing random images"
      ],
      "metadata": {
        "id": "CX5rJ6ofRFaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to import an image and resize it to be able to be used with our model\n",
        "def load_and_prep_image(filename, img_shape=224):\n",
        "  \"\"\"\n",
        "  Reads an image from filename, turns it into a tensor\n",
        "  and reshapes it to (img_shape, img_shape, colour_channel).\n",
        "  \"\"\"\n",
        "  # Read in target file (an image)\n",
        "  img = tf.io.read_file(filename)\n",
        "\n",
        "  # Decode the read file into a tensor & ensure 3 colour channels \n",
        "  # (our model is trained on images with 3 colour channels and sometimes images have 4 colour channels)\n",
        "  img = tf.image.decode_image(img, channels=3)\n",
        "\n",
        "  # Resize the image (to the same size our model was trained on)\n",
        "  img = tf.image.resize(img, size = [img_shape, img_shape])\n",
        "\n",
        "  # Rescale the image (get all values between 0 and 1)\n",
        "  img = img/255.\n",
        "  return img\n",
        "\n",
        "#####################################################################################\n",
        "  # Load in and preprocess our custom image\n",
        "steak = load_and_prep_image(\"03-steak.jpeg\")\n",
        "steak"
      ],
      "metadata": {
        "id": "bjmtj4GJRLvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hYIB4vq8erHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###this is for test results to visualize prediction accuracy\n",
        "import random\n",
        "\n",
        "# Create a function for plotting a random image along with its prediction\n",
        "def plot_random_image(model, images, true_labels, classes):\n",
        "  \"\"\"Picks a random image, plots it and labels it with a predicted and truth label.\n",
        "\n",
        "  Args:\n",
        "    model: a trained model (trained on data similar to what's in images).\n",
        "    images: a set of random images (in tensor form).\n",
        "    true_labels: array of ground truth labels for images.\n",
        "    classes: array of class names for images.\n",
        "  \n",
        "  Returns:\n",
        "    A plot of a random image from `images` with a predicted class label from `model`\n",
        "    as well as the truth class label from `true_labels`.\n",
        "  \"\"\" \n",
        "  # Setup random integer\n",
        "  i = random.randint(0, len(images))\n",
        "  \n",
        "  # Create predictions and targets\n",
        "  target_image = images[i]\n",
        "  pred_probs = model.predict(target_image.reshape(1, 28, 28)) # have to reshape to get into right size for model\n",
        "  pred_label = classes[pred_probs.argmax()]\n",
        "  true_label = classes[true_labels[i]]\n",
        "\n",
        "  # Plot the target image\n",
        "  plt.imshow(target_image, cmap=plt.cm.binary)\n",
        "\n",
        "  # Change the color of the titles depending on if the prediction is right or wrong\n",
        "  if pred_label == true_label:\n",
        "    color = \"green\"\n",
        "  else:\n",
        "    color = \"red\"\n",
        "\n",
        "  # Add xlabel information (prediction/true label)\n",
        "  plt.xlabel(\"Pred: {} {:2.0f}% (True: {})\".format(pred_label,\n",
        "                                                   100*tf.reduce_max(pred_probs),\n",
        "                                                   true_label),\n",
        "             color=color) # set the color to green or red"
      ],
      "metadata": {
        "id": "iq7ZZz6lNRVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this is to view a random image from array of images\n",
        "# Show original image and augmented image\n",
        "random_number = random.randint(0, 32) # we're making batches of size 32, so we'll get a random instance\n",
        "plt.imshow(images[random_number])\n",
        "plt.title(f\"Original image\")\n",
        "plt.axis(False)\n",
        "plt.figure()\n",
        "plt.imshow(augmented_images[random_number])\n",
        "plt.title(f\"Augmented image\")\n",
        "plt.axis(False);"
      ],
      "metadata": {
        "id": "mBdYsr-6dsor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###convolution"
      ],
      "metadata": {
        "id": "p4lVIlJbaEfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#architecture of a basic convolution neural network\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Set the seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Preprocess data (get all of the pixel values between 1 and 0, also called scaling/normalization)\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Setup the train and test directories\n",
        "train_dir = \"pizza_steak/train/\"\n",
        "test_dir = \"pizza_steak/test/\"\n",
        "\n",
        "# Import data from directories and turn it into batches\n",
        "train_data = train_datagen.flow_from_directory(train_dir,\n",
        "                                               batch_size=32, # number of images to process at a time \n",
        "                                               target_size=(224, 224), # convert all images to be 224 x 224\n",
        "                                               class_mode=\"binary\", # type of problem we're working on\n",
        "                                               seed=42)\n",
        "\n",
        "valid_data = valid_datagen.flow_from_directory(test_dir,\n",
        "                                               batch_size=32,\n",
        "                                               target_size=(224, 224),\n",
        "                                               class_mode=\"binary\",\n",
        "                                               seed=42)\n",
        "\n",
        "# Create a CNN model (same as Tiny VGG - https://poloclub.github.io/cnn-explainer/)\n",
        "model_1 = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(filters=10, \n",
        "                         kernel_size=3, # can also be (3, 3)\n",
        "                         activation=\"relu\", \n",
        "                         input_shape=(224, 224, 3)), # first layer specifies input shape (height, width, colour channels)\n",
        "  tf.keras.layers.Conv2D(10, 3, activation=\"relu\"),\n",
        "  tf.keras.layers.MaxPool2D(pool_size=2, # pool_size can also be (2, 2)\n",
        "                            padding=\"valid\"), # padding can also be 'same'\n",
        "  tf.keras.layers.Conv2D(10, 3, activation=\"relu\"),\n",
        "  tf.keras.layers.Conv2D(10, 3, activation=\"relu\"), # activation='relu' == tf.keras.layers.Activations(tf.nn.relu)\n",
        "  tf.keras.layers.MaxPool2D(2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(1, activation=\"sigmoid\") # binary activation output\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_1.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# Fit the model\n",
        "history_1 = model_1.fit(train_data,\n",
        "                        epochs=5,\n",
        "                        steps_per_epoch=len(train_data),\n",
        "                        validation_data=valid_data,\n",
        "                        validation_steps=len(valid_data))"
      ],
      "metadata": {
        "id": "7-zA3_V_aMjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###evaluating and visualizing models"
      ],
      "metadata": {
        "id": "6-mueSIbcvUc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####plotting the loss curves"
      ],
      "metadata": {
        "id": "fZgY4qa7c1OU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the validation and training data separately\n",
        "def plot_loss_curves(history):\n",
        "  \"\"\"\n",
        "  Returns separate loss curves for training and validation metrics.\n",
        "  \"\"\" \n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "\n",
        "  accuracy = history.history['accuracy']\n",
        "  val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "  epochs = range(len(history.history['loss']))\n",
        "\n",
        "  # Plot loss\n",
        "  plt.plot(epochs, loss, label='training_loss')\n",
        "  plt.plot(epochs, val_loss, label='val_loss')\n",
        "  plt.title('Loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.legend()\n",
        "\n",
        "  # Plot accuracy\n",
        "  plt.figure()\n",
        "  plt.plot(epochs, accuracy, label='training_accuracy')\n",
        "  plt.plot(epochs, val_accuracy, label='val_accuracy')\n",
        "  plt.title('Accuracy')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.legend();"
      ],
      "metadata": {
        "id": "jdtjBJhuc6hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust function to work with multi-class\n",
        "def pred_and_plot(model, filename, class_names):\n",
        "  \"\"\"\n",
        "  Imports an image located at filename, makes a prediction on it with\n",
        "  a trained model and plots the image with the predicted class as the title.\n",
        "  \"\"\"\n",
        "  # Import the target image and preprocess it\n",
        "  img = load_and_prep_image(filename)\n",
        "\n",
        "  # Make a prediction\n",
        "  pred = model.predict(tf.expand_dims(img, axis=0))\n",
        "\n",
        "  # Get the predicted class\n",
        "  if len(pred[0]) > 1: # check for multi-class\n",
        "    pred_class = class_names[pred.argmax()] # if more than one output, take the max\n",
        "  else:\n",
        "    pred_class = class_names[int(tf.round(pred)[0][0])] # if only one output, round\n",
        "\n",
        "  # Plot the image and predicted class\n",
        "  plt.imshow(img)\n",
        "  plt.title(f\"Prediction: {pred_class}\")\n",
        "  plt.axis(False);"
      ],
      "metadata": {
        "id": "QlESdejcfHkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###saving and loading models"
      ],
      "metadata": {
        "id": "LWHJmDbsgTdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save a model\n",
        "model_11.save(\"saved_trained_model\")\n",
        "\n",
        "###############################\n",
        "# Load in a model and evaluate it\n",
        "loaded_model_11 = tf.keras.models.load_model(\"saved_trained_model\")\n",
        "loaded_model_11.evaluate(test_data)\n",
        "\n",
        "###################################\n",
        "# Compare our unsaved model's results (same as above)\n",
        "model_11.evaluate(test_data)"
      ],
      "metadata": {
        "id": "-i-G9DrigWg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###find out how many images in a file"
      ],
      "metadata": {
        "id": "i4Rc-pngXQhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Another way to find out how many images are in a file\n",
        "num_steak_images_train = len(os.listdir(\"pizza_steak/train/steak\"))\n",
        "\n",
        "num_steak_images_train"
      ],
      "metadata": {
        "id": "ZKmCv_vJXUaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################NLP"
      ],
      "metadata": {
        "id": "u5waqu-JNYgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's visualize some random training examples\n",
        "import random\n",
        "random_index = random.randint(0, len(train_df)-5) # create random indexes not higher than the total number of samples\n",
        "for row in train_df_shuffled[[\"text\", \"target\"]][random_index:random_index+5].itertuples():\n",
        "  _, text, target = row\n",
        "  print(f\"Target: {target}\", \"(real disaster)\" if target > 0 else \"(not real disaster)\")\n",
        "  print(f\"Text:\\n{text}\\n\")\n",
        "  print(\"---\\n\")"
      ],
      "metadata": {
        "id": "x259wF5DNWn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use train_test_split to split training data into training and validation sets\n",
        "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled[\"text\"].to_numpy(),\n",
        "                                                                            train_df_shuffled[\"target\"].to_numpy(),\n",
        "                                                                            test_size=0.1, # dedicate 10% of samples to validation set\n",
        "                                                                            random_state=42) # random state for reproducibility"
      ],
      "metadata": {
        "id": "Zqn5L-N8NnuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the lengths\n",
        "len(train_sentences), len(train_labels), len(val_sentences), len(val_labels)"
      ],
      "metadata": {
        "id": "-Num6P0CNvo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the first 10 training sentences and their labels\n",
        "train_sentences[:10], train_labels[:10]"
      ],
      "metadata": {
        "id": "5m5U4NkfNw8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN-houoSD-hP"
      },
      "source": [
        "## Converting text into numbers\n",
        "\n",
        "Wonderful! We've got a training set and a validation set containing Tweets and labels.\n",
        "\n",
        "Our labels are in numerical form (`0` and `1`) but our Tweets are in string form.\n",
        "\n",
        "> 🤔 **Question:** What do you think we have to do before we can use a machine learning algorithm with our text data? \n",
        "\n",
        "If you answered something along the lines of \"turn it into numbers\", you're correct. A machine learning algorithm requires its inputs to be in numerical form.\n",
        "\n",
        "In NLP, there are two main concepts for turning text into numbers:\n",
        "* **Tokenization** - A straight mapping from word or character or sub-word to a numerical value. There are three main levels of tokenization:\n",
        "  1. Using **word-level tokenization** with the sentence \"I love TensorFlow\" might result in \"I\" being `0`, \"love\" being `1` and \"TensorFlow\" being `2`. In this case, every word in a sequence considered a single **token**.\n",
        "  2. **Character-level tokenization**, such as converting the letters A-Z to values `1-26`. In this case, every character in a sequence considered a single **token**.\n",
        "  3. **Sub-word tokenization** is in between word-level and character-level tokenization. It involves breaking invidual words into smaller parts and then converting those smaller parts into numbers. For example, \"my favourite food is pineapple pizza\" might become \"my, fav, avour, rite, fo, oo, od, is, pin, ine, app, le, piz, za\". After doing this, these sub-words would then be mapped to a numerical value. In this case, every word could be considered multiple **tokens**.\n",
        "* **Embeddings** - An embedding is a representation of natural language which can be learned. Representation comes in the form of a **feature vector**. For example, the word \"dance\" could be represented by the 5-dimensional vector `[-0.8547, 0.4559, -0.3332, 0.9877, 0.1112]`. It's important to note here, the size of the feature vector is tuneable. There are two ways to use embeddings: \n",
        "  1. **Create your own embedding** - Once your text has been turned into numbers (required for an embedding), you can put them through an embedding layer (such as [`tf.keras.layers.Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding)) and an embedding representation will be learned during model training.\n",
        "  2. **Reuse a pre-learned embedding** - Many pre-trained embeddings exist online. These pre-trained embeddings have often been learned on large corpuses of text (such as all of Wikipedia) and thus have a good underlying representation of natural language. You can use a pre-trained embedding to initialize your model and fine-tune it to your own specific task.\n",
        "\n",
        "![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/08-tokenization-vs-embedding.png)\n",
        "*Example of **tokenization** (straight mapping from word to number) and **embedding** (richer representation of relationships between tokens).*\n",
        "\n",
        "> 🤔 **Question:** What level of tokenzation should I use? What embedding should should I choose?\n",
        "\n",
        "It depends on your problem. You could try character-level tokenization/embeddings and word-level tokenization/embeddings and see which perform best. You might even want to try stacking them (e.g. combining the outputs of your embedding layers using [`tf.keras.layers.concatenate`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/concatenate)). \n",
        "\n",
        "If you're looking for pre-trained word embeddings, [Word2vec embeddings](http://jalammar.github.io/illustrated-word2vec/), [GloVe embeddings](https://nlp.stanford.edu/projects/glove/) and many of the options available on [TensorFlow Hub](https://tfhub.dev/s?module-type=text-embedding) are great places to start.\n",
        "\n",
        "> 🔑 **Note:** Much like searching for a pre-trained computer vision model, you can search for pre-trained word embeddings to use for your problem. Try searching for something like \"use pre-trained word embeddings in TensorFlow\"."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "# Note: in TensorFlow 2.6+, you no longer need \"layers.experimental.preprocessing\"\n",
        "# you can use: \"tf.keras.layers.TextVectorization\", see https://github.com/tensorflow/tensorflow/releases/tag/v2.6.0 for more\n",
        "\n",
        "# Use the default TextVectorization variables\n",
        "text_vectorizer = TextVectorization(max_tokens=None, # how many words in the vocabulary (all of the different words in your text)\n",
        "                                    standardize=\"lower_and_strip_punctuation\", # how to process text\n",
        "                                    split=\"whitespace\", # how to split tokens\n",
        "                                    ngrams=None, # create groups of n-words?\n",
        "                                    output_mode=\"int\", # how to map tokens to numbers\n",
        "                                    output_sequence_length=None) # how long should the output sequence of tokens be?\n",
        "                                    # pad_to_max_tokens=True) # Not valid if using max_tokens=None"
      ],
      "metadata": {
        "id": "NopXP9jAP6SZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the unique words in the vocabulary\n",
        "words_in_vocab = text_vectorizer.get_vocabulary()\n",
        "top_5_words = words_in_vocab[:5] # most common tokens (notice the [UNK] token for \"unknown\" words)\n",
        "bottom_5_words = words_in_vocab[-5:] # least common tokens\n",
        "print(f\"Number of words in vocab: {len(words_in_vocab)}\")\n",
        "print(f\"Top 5 most common words: {top_5_words}\") \n",
        "print(f\"Bottom 5 least common words: {bottom_5_words}\")"
      ],
      "metadata": {
        "id": "d1m3VcdWQ-TW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "embedding = layers.Embedding(input_dim=max_vocab_length, # set input shape\n",
        "                             output_dim=128, # set size of embedding vector\n",
        "                             embeddings_initializer=\"uniform\", # default, intialize randomly\n",
        "                             input_length=max_length, # how long is each input\n",
        "                             name=\"embedding_1\") \n",
        "\n",
        "embedding"
      ],
      "metadata": {
        "id": "EvYWLCgKRg7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn prediction probabilities into single-dimension tensor of floats ##from a dense model\n",
        "model_1_preds = tf.squeeze(tf.round(model_1_pred_probs)) # squeeze removes single dimensions\n",
        "model_1_preds[:20]"
      ],
      "metadata": {
        "id": "l_6fGoBuYEKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate: accuracy, precision, recall, f1-score\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "def calculate_results(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Calculates model accuracy, precision, recall and f1 score of a binary classification model.\n",
        "\n",
        "  Args:\n",
        "  -----\n",
        "  y_true = true labels in the form of a 1D array\n",
        "  y_pred = predicted labels in the form of a 1D array\n",
        "\n",
        "  Returns a dictionary of accuracy, precision, recall, f1-score.\n",
        "  \"\"\"\n",
        "  # Calculate model accuracy\n",
        "  model_accuracy = accuracy_score(y_true, y_pred) * 100\n",
        "  # Calculate model precision, recall and f1 score using \"weighted\" average\n",
        "  model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
        "  model_results = {\"accuracy\": model_accuracy,\n",
        "                  \"precision\": model_precision,\n",
        "                  \"recall\": model_recall,\n",
        "                  \"f1\": model_f1}\n",
        "  return model_results"
      ],
      "metadata": {
        "id": "zgioI2ZvTlmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get baseline results\n",
        "baseline_results = calculate_results(y_true=val_labels,\n",
        "                                     y_pred=baseline_preds)\n",
        "baseline_results"
      ],
      "metadata": {
        "id": "FDvVu7RlTpYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed and create embedding layer (new embedding layer for each model)\n",
        "tf.random.set_seed(42)\n",
        "from tensorflow.keras import layers\n",
        "model_2_embedding = layers.Embedding(input_dim=max_vocab_length,\n",
        "                                     output_dim=128,\n",
        "                                     embeddings_initializer=\"uniform\",\n",
        "                                     input_length=max_length,\n",
        "                                     name=\"embedding_2\")\n",
        "\n",
        "\n",
        "# Create LSTM model\n",
        "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
        "x = text_vectorizer(inputs)\n",
        "x = model_2_embedding(x)\n",
        "print(x.shape)\n",
        "# x = layers.LSTM(64, return_sequences=True)(x) # return vector for each word in the Tweet (you can stack RNN cells as long as return_sequences=True)\n",
        "x = layers.LSTM(64)(x) # return vector for whole sequence\n",
        "print(x.shape)\n",
        "# x = layers.Dense(64, activation=\"relu\")(x) # optional dense layer on top of output of LSTM cell\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model_2 = tf.keras.Model(inputs, outputs, name=\"model_2_LSTM\")"
      ],
      "metadata": {
        "id": "BYXTjAOTcSD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################splittng already split data from training set to get a subset for transfer learnin"
      ],
      "metadata": {
        "id": "5IGOECOHjsRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One kind of correct way (there are more) to make data subset\n",
        "# (split the already split train_sentences/train_labels)\n",
        "train_sentences_90_percent, train_sentences_10_percent, train_labels_90_percent, train_labels_10_percent = train_test_split(np.array(train_sentences),\n",
        "                                                                                                                            train_labels,\n",
        "                                                                                                                            test_size=0.1,\n",
        "                                                                                                                            random_state=42)\n"
      ],
      "metadata": {
        "id": "lmPd8nttjyi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check length of 10 percent datasets\n",
        "print(f\"Total training examples: {len(train_sentences)}\")\n",
        "print(f\"Length of 10% training examples: {len(train_sentences_10_percent)}\")"
      ],
      "metadata": {
        "id": "AodRs1_Zjz3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_on_sentence(model, sentence):\n",
        "  \"\"\"\n",
        "  Uses model to make a prediction on sentence.\n",
        "\n",
        "  Returns the sentence, the predicted label and the prediction probability.\n",
        "  \"\"\"\n",
        "  pred_prob = model.predict([sentence])\n",
        "  pred_label = tf.squeeze(tf.round(pred_prob)).numpy()\n",
        "  print(f\"Pred: {pred_label}\", \"(real disaster)\" if pred_label > 0 else \"(not real disaster)\", f\"Prob: {pred_prob[0][0]}\")\n",
        "  print(f\"Text:\\n{sentence}\")"
      ],
      "metadata": {
        "id": "7Jco_-X8mogZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a prediction on Tweet from the wild\n",
        "predict_on_sentence(model=model_6, # use the USE model\n",
        "                    sentence=daniels_tweet)"
      ],
      "metadata": {
        "id": "ZdIEvaPYmqzA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}